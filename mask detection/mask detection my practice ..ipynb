{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\programdata\\anaconda3\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: imutils in c:\\programdata\\anaconda3\\lib\\site-packages (0.5.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement libwebp (from versions: none)\n",
      "ERROR: No matching distribution found for libwebp\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install imutils\n",
    "!pip install libwebp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the libraries and modules required.\n",
    "\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import imutils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the neural network:\n",
    "\n",
    "#This convolution network consists of two pairs of Conv and MaxPool layers to extract features from the dataset. Which is then followed by a Flatten and Dropout layer to convert the data in 1D and ensure overfitting.\n",
    "\n",
    "#And then two Dense layers for classificationmodel = Sequential([\n",
    "    Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(100, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 604 images belonging to 2 classes.\n",
      "Found 52 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#3. Image Data Generation/Augmentation:\n",
    "\n",
    "\n",
    "TRAINING_DIR = r\"C:/Users/Toyaz Pandey/Downloads/Dataset/train\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(150, 150))\n",
    "VALIDATION_DIR = r\"C:/Users/Toyaz Pandey/Downloads/Dataset/test\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=10, \n",
    "                                                         target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#4. Initialize a callback checkpoint to keep saving best model after each epoch while training:\n",
    "\n",
    "checkpoint = ModelCheckpoint('model2-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-6272b23e5a0b>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8853 - acc: 0.4603WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model2-001.model\\assets\n",
      "61/61 [==============================] - 59s 971ms/step - loss: 0.8853 - acc: 0.4603 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - 56s 913ms/step - loss: 0.6930 - acc: 0.5199 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6928 - acc: 0.5149INFO:tensorflow:Assets written to: model2-003.model\\assets\n",
      "61/61 [==============================] - 59s 965ms/step - loss: 0.6928 - acc: 0.5149 - val_loss: 0.6921 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6871 - acc: 0.5050INFO:tensorflow:Assets written to: model2-004.model\\assets\n",
      "61/61 [==============================] - 58s 952ms/step - loss: 0.6871 - acc: 0.5050 - val_loss: 0.6812 - val_acc: 0.4615\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6876 - acc: 0.5281INFO:tensorflow:Assets written to: model2-005.model\\assets\n",
      "61/61 [==============================] - 59s 962ms/step - loss: 0.6876 - acc: 0.5281 - val_loss: 0.6692 - val_acc: 0.5385\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - 47s 772ms/step - loss: 0.6747 - acc: 0.5381 - val_loss: 0.6799 - val_acc: 0.6346\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6498 - acc: 0.6093INFO:tensorflow:Assets written to: model2-007.model\\assets\n",
      "61/61 [==============================] - 72s 1s/step - loss: 0.6498 - acc: 0.6093 - val_loss: 0.4473 - val_acc: 0.7308\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.5790 - acc: 0.7020INFO:tensorflow:Assets written to: model2-008.model\\assets\n",
      "61/61 [==============================] - 77s 1s/step - loss: 0.5790 - acc: 0.7020 - val_loss: 0.3493 - val_acc: 0.8846\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.4942 - acc: 0.7831INFO:tensorflow:Assets written to: model2-009.model\\assets\n",
      "61/61 [==============================] - 68s 1s/step - loss: 0.4942 - acc: 0.7831 - val_loss: 0.2448 - val_acc: 0.9038\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.4931 - acc: 0.7930INFO:tensorflow:Assets written to: model2-010.model\\assets\n",
      "61/61 [==============================] - 60s 978ms/step - loss: 0.4931 - acc: 0.7930 - val_loss: 0.2148 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#5. Train the model:\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=10,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bdadafd2ecd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mrerect_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mrect_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mrect_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhaarcascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrerect_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrect_size\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "model=load_model(\"./model2-001.model\")\n",
    "results={0:'without mask',1:'mask'}\n",
    "GR_dict={0:(0,0,255),1:(0,255,0)}\n",
    "rect_size = 4\n",
    "cap = cv2.VideoCapture(0) \n",
    "haarcascade = cv2.CascadeClassifier('C:/Users/Toyaz Pandey/.local/lib/python3.6/site-packages/cv2/data/haarcascade_frontalface_default.xml')\n",
    "while True:\n",
    "    (rval, im) = cap.read()\n",
    "    im=cv2.flip(im,1,1) \n",
    "    \n",
    "    rerect_size = cv2.resize(im, (im.shape[1] // rect_size, im.shape[0] // rect_size))\n",
    "    faces = haarcascade.detectMultiScale(rerect_size)\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * rect_size for v in f] \n",
    "        \n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        rerect_sized=cv2.resize(face_img,(150,150))\n",
    "        normalized=rerect_sized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),GR_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),GR_dict[label],-1)\n",
    "        cv2.putText(im, results[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    \n",
    "    if key == 27: \n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
